{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd706WOjOQzp"
   },
   "source": [
    "# Практическое задание 2\n",
    "\n",
    "\n",
    "\n",
    "## Замечания\n",
    "\n",
    "* Задание необходимо сдать боту до 06.12.2021\n",
    "* Соблюдаем кодекс чести (по нулям и списавшему, и давшему списать)\n",
    "* Можно (и нужно!) применять для реализации только библиотеку **Numpy**\n",
    "* Ничего, крому Numpy, нельзя использовать для реализации \n",
    "* **Keras** используется только для тестирования Вашей реализации\n",
    "* Если какой-то из классов не проходит приведенные тесты, то соответствующее задание не оценивается\n",
    "* Возможно использование дополнительных (приватных) тестов\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNPPcPjjOQzr"
   },
   "source": [
    "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1l6be3YOQzr"
   },
   "source": [
    "Задание состоит из трёх частей:\n",
    "1. Реализация прямого вывода нейронной сети (первое практическое задание)\n",
    "2. Реализация градиентов по входу и распространения градиента по сети (back propagation)\n",
    "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOR73ppqOQzr"
   },
   "source": [
    "###  1. Реализация вывода собственной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sd8psO5OQzr"
   },
   "source": [
    "1.1 Внимательно ознакомьтесь с интерфейсом слоя. Любой слой должен содержать как минимум три метода:\n",
    "- конструктор\n",
    "- прямой вывод \n",
    "- обратный вывод, производные по входу и по параметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ACEEU6sOQzs"
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"Layer\"\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "\n",
    "    def backward(self, input_data):\n",
    "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "        pass\n",
    "\n",
    "    def grad_param(self, input_data):\n",
    "        return []\n",
    "\n",
    "    def update_param(self, grads, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4_u58B-OQzs"
   },
   "source": [
    "1.2 Ниже предствален интерфейс класса  Network. Обратите внимание на реализацию метода predict, который последовательно обрабатывает входные данные слой за слоем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "moEqx2hjOQzs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, layers, loss=None):\n",
    "        self.name = \"Network\"\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        return self.predict(input_data)\n",
    "\n",
    "    def grad_x(self, input_data, labels):\n",
    "        temp_input = input_data\n",
    "        grad_layers = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            grad_layers.append(layer.grad_x(temp_input))\n",
    "            temp_input = layer.forward(temp_input)\n",
    "\n",
    "        loss_grad = self.loss.grad_x(temp_input, labels)  # по софтмаксу и лейблам\n",
    "        total_grad = loss_grad.copy()\n",
    "        total_grad = np.expand_dims(total_grad, 1)\n",
    "\n",
    "        for grad in grad_layers[::-1]:\n",
    "            total_grad = total_grad @ grad\n",
    "\n",
    "        return np.squeeze(total_grad)\n",
    "\n",
    "    def grad_param(self, input_data, labels):\n",
    "        weights_grad = []\n",
    "        temp_w_grads = []\n",
    "        temp_input = input_data\n",
    "        grad_layers = []\n",
    "        for layer in self.layers:\n",
    "            grad_layers.append(layer.grad_x(temp_input))\n",
    "            if layer.name == \"Dense\":\n",
    "                temp_w_grads.append(\n",
    "                    (layer.grad_W(temp_input), layer.grad_b(temp_input))\n",
    "                )\n",
    "            else:\n",
    "                temp_w_grads.append(None)\n",
    "            temp_input = layer.forward(temp_input)\n",
    "        loss_grad = self.loss.grad_x(temp_input, labels)\n",
    "        total_grad = loss_grad.copy()\n",
    "        total_grad = np.expand_dims(total_grad, 1)\n",
    "\n",
    "        for i, layer in enumerate(self.layers[::-1]):\n",
    "            if layer.name == \"Dense\":\n",
    "                w_grad = total_grad @ temp_w_grads[len(self.layers) - 1 - i][0]\n",
    "                b_grad = total_grad @ temp_w_grads[len(self.layers) - 1 - i][1]\n",
    "                b_grad = np.squeeze(b_grad)\n",
    "                w_grad = np.reshape(w_grad, (w_grad.shape[0], -1, b_grad.shape[1]))\n",
    "                weights_grad.append((w_grad.mean(axis=0), b_grad.mean(axis=0)))\n",
    "            else:\n",
    "                weights_grad.append(None)\n",
    "            total_grad = total_grad @ grad_layers[len(grad_layers) - 1 - i]\n",
    "\n",
    "        return weights_grad[::-1]\n",
    "\n",
    "    def update(self, grad_list, learning_rate):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if layer.name == \"Dense\":\n",
    "                layer.W = layer.W - learning_rate * grad_list[i][0]\n",
    "                layer.b = layer.b - learning_rate * grad_list[i][1]\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        current_input = input_data\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.forward(current_input)\n",
    "        return current_input\n",
    "\n",
    "    def calculate_loss(self, input_data, labels):\n",
    "        return self.loss.forward(self.predict(input_data), labels)\n",
    "\n",
    "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
    "        grad_list = self.grad_param(input_data, labels)\n",
    "        self.update(grad_list, learning_rate)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        trainX,\n",
    "        trainY,\n",
    "        validation_split=0.25,\n",
    "        batch_size=1,\n",
    "        nb_epoch=1,\n",
    "        learning_rate=0.01,\n",
    "    ):\n",
    "\n",
    "        train_x, val_x, train_y, val_y = train_test_split(\n",
    "            trainX, trainY, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        for epoch in range(nb_epoch):\n",
    "            # train one epoch\n",
    "            for i in tqdm(range(int(len(train_x) / batch_size))):\n",
    "                batch_x = train_x[i * batch_size : (i + 1) * batch_size]\n",
    "                batch_y = train_y[i * batch_size : (i + 1) * batch_size]\n",
    "                self.train_step(batch_x, batch_y, learning_rate)\n",
    "            # validate\n",
    "            val_accuracy = self.evaluate(val_x, val_y)\n",
    "            print(\"%d epoch: val %.2f\" % (epoch + 1, val_accuracy))\n",
    "\n",
    "    def evaluate(self, testX, testY):\n",
    "        y_pred = np.argmax(self.predict(testX), axis=1)\n",
    "        y_true = np.argmax(testY, axis=1)\n",
    "        val_accuracy = np.sum((y_pred == y_true)) / (len(y_true))\n",
    "        return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY1vQQsyOQzs"
   },
   "source": [
    "#### 1.1 (6 баллов) Необходимо реализовать метод forward для вычисления следующих слоёв:\n",
    "\n",
    "- DenseLayer\n",
    "- ReLU\n",
    "- Softmax\n",
    "- FlattenLayer\n",
    "- MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UTPCJ9VzOQzs"
   },
   "outputs": [],
   "source": [
    "# импорты\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4JYeJax4OQzt"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
    "        self.name = \"Dense\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        if W_init is None or b_init is None:\n",
    "            self.W = np.random.uniform(\n",
    "                low=(-np.sqrt(6) / np.sqrt(input_dim + output_dim)),\n",
    "                high=(np.sqrt(6) / np.sqrt(input_dim + output_dim)),\n",
    "                size=(input_dim, output_dim),\n",
    "            )\n",
    "            self.b = np.zeros(output_dim, \"float32\")\n",
    "        else:\n",
    "            self.W = W_init\n",
    "            self.b = b_init\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        assert input_data.shape[1] == self.W.shape[0], \"Mismatch in dimensions\"\n",
    "        out = input_data @ self.W + self.b\n",
    "        return out\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "        batch_size = input_data.shape[0]\n",
    "        ans = np.empty((input_data.shape[0], self.W.shape[1], self.W.shape[0]))\n",
    "        for i in range(ans.shape[0]):\n",
    "            ans[i] = np.copy(self.W.T)\n",
    "        return ans\n",
    "\n",
    "        return self.W.T\n",
    "\n",
    "    def grad_b(self, input_data):\n",
    "        ans = np.empty((input_data.shape[0], len(self.b), len(self.b)))\n",
    "        for i in range(len(ans)):\n",
    "            ans[i] = np.eye(len(self.b))\n",
    "        return ans\n",
    "\n",
    "    def grad_W(self, input_data):\n",
    "        ans = np.zeros(\n",
    "            (len(input_data), self.W.shape[1], self.W.shape[0] * self.W.shape[1])\n",
    "        )\n",
    "        step = self.W.shape[1]\n",
    "        for b in range(input_data.shape[0]):\n",
    "            for i in range(self.W.shape[1]):\n",
    "                for j in range(input_data.shape[1]):\n",
    "                    ans[b][i][i + j * step] = input_data[b][j]\n",
    "        return ans\n",
    "\n",
    "    def update_W(self, grad, learning_rate):\n",
    "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
    "\n",
    "    def update_b(self, grad, learning_rate):\n",
    "        self.b -= learning_rate * np.mean(grad, axis=0)\n",
    "\n",
    "    def update_param(self, params_grad, learning_rate):\n",
    "        self.update_W(params_grad[0], learning_rate)\n",
    "        self.update_b(params_grad[1], learning_rate)\n",
    "\n",
    "    def grad_param(self, input_data):\n",
    "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = \"ReLU\"\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        input_data[input_data < 0] = 0.0\n",
    "        return input_data\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "        ans = np.zeros((input_data.shape[0], input_data.shape[1], input_data.shape[1]))\n",
    "        for b in range(ans.shape[0]):\n",
    "            for i in range(ans.shape[1]):\n",
    "                if input_data[b][i] > 0:\n",
    "                    ans[b][i][i] = 1.0\n",
    "        return ans\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = \"Softmax\"\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        for i in range(len(input_data)):\n",
    "            denominator = sum(np.exp(input_data[i]))\n",
    "            input_data[i] = np.exp(input_data[i]) / denominator\n",
    "        return input_data\n",
    "\n",
    "    def grad_x(self, input_data):\n",
    "        ans = np.empty((input_data.shape[0], input_data.shape[1], input_data.shape[1]))\n",
    "        for b in range(ans.shape[0]):\n",
    "            denominator = sum(np.exp(input_data[b]))\n",
    "            for i in range(ans.shape[1]):\n",
    "                for j in range(ans.shape[2]):\n",
    "                    if i == j:\n",
    "                        ans[b][i][j] = (\n",
    "                            np.exp(input_data[b][i])\n",
    "                            / denominator\n",
    "                            * (denominator - np.exp(input_data[b][j]))\n",
    "                            / denominator\n",
    "                        )\n",
    "                    else:\n",
    "                        ans[b][i][j] = (\n",
    "                            -np.exp(input_data[b][i])\n",
    "                            / denominator\n",
    "                            * np.exp(input_data[b][j])\n",
    "                            / denominator\n",
    "                        )\n",
    "\n",
    "        return np.array(ans)\n",
    "\n",
    "\n",
    "class FlattenLayer(Layer):\n",
    "    def __init__(self):\n",
    "        self.name = \"Flatten\"\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        data = np.swapaxes(input_data, 1, 3)\n",
    "        out = np.reshape(np.swapaxes(data, 1, 2), (data.shape[0], -1))\n",
    "        return out\n",
    "\n",
    "    def grad_x(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MaxPooling(Layer):\n",
    "    def __init__(self, pool_size=(2, 2), strides=2):\n",
    "        self.name = \"MaxPooling\"\n",
    "        self.pool_size = pool_size\n",
    "        self.strides = 2\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        out_h = int(\n",
    "            (input_data.shape[2] - (self.pool_size[0] - 1) - 1) / self.strides + 1\n",
    "        )\n",
    "        out_w = int(\n",
    "            (input_data.shape[3] - (self.pool_size[1] - 1) - 1) / self.strides + 1\n",
    "        )\n",
    "        out = np.empty((input_data.shape[0], input_data.shape[1], out_h, out_w))\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                out[:, :, i, j] = np.max(\n",
    "                    input_data[\n",
    "                        :,\n",
    "                        :,\n",
    "                        i * self.strides : i * self.strides + self.pool_size[0],\n",
    "                        j * self.strides : j * self.strides + self.pool_size[1],\n",
    "                    ],\n",
    "                    axis=(2, 3),\n",
    "                )\n",
    "        return out\n",
    "\n",
    "    def grad_x(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiCuhpycOQzt"
   },
   "source": [
    "#### 1.2 (3 балла) Реализуйте теперь свёртночный слой   (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J7JpnsQXOQzt"
   },
   "outputs": [],
   "source": [
    "class Conv2D(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        padding=\"valid\",\n",
    "        stride=1,\n",
    "        kernels_init=None,\n",
    "        bias_init=None,\n",
    "    ):\n",
    "        self.name = \"Conv2D\"\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        if kernels_init is None or bias_init is None:\n",
    "            pass\n",
    "        else:\n",
    "            self.kernel = kernels_init\n",
    "            self.bias = bias_init\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        if self.padding == \"same\":\n",
    "            if input_data.shape[2] % self.stride == 0:\n",
    "                pad_h = max(self.kernel_size - self.stride, 0)\n",
    "            else:\n",
    "                pad_h = max(self.kernel_size - (input_data.shape[2] % self.stride), 0)\n",
    "            if input_data.shape[3] % self.stride == 0:\n",
    "                pad_w = max(self.kernel_size - self.stride, 0)\n",
    "            else:\n",
    "                pad_w = max(self.kernel_size - (input_data.shape[3] % self.stride), 0)\n",
    "            pad_top = pad_h // 2\n",
    "            pad_bottom = pad_h - pad_top\n",
    "            pad_left = pad_w // 2\n",
    "            pad_right = pad_w - pad_left\n",
    "            temp_input_data = np.pad(\n",
    "                input_data,\n",
    "                ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)),\n",
    "            )\n",
    "            temp_input_data[\n",
    "                :,\n",
    "                :,\n",
    "                pad_top : pad_top + input_data.shape[2],\n",
    "                pad_left : pad_left + input_data.shape[3],\n",
    "            ] = input_data\n",
    "        else:\n",
    "            temp_input_data = input_data\n",
    "\n",
    "        out_h = (\n",
    "            temp_input_data.shape[2] - self.kernel_size + self.stride\n",
    "        ) // self.stride\n",
    "        out_w = (\n",
    "            temp_input_data.shape[3] - self.kernel_size + self.stride\n",
    "        ) // self.stride\n",
    "        out = np.zeros((input_data.shape[0], self.output_channels, out_h, out_w))\n",
    "        for oc in range(self.output_channels):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    out[:, oc, i, j] = np.tensordot(\n",
    "                        temp_input_data[\n",
    "                            :,\n",
    "                            :,\n",
    "                            i * self.stride : i * self.stride + self.kernel_size,\n",
    "                            j * self.stride : j * self.stride + self.kernel_size,\n",
    "                        ],\n",
    "                        self.kernel[:, :, :, oc],\n",
    "                        axes=([2, 3, 1], [0, 1, 2]),\n",
    "                    )\n",
    "\n",
    "            out[:, oc] += self.bias[oc]\n",
    "        return out\n",
    "\n",
    "    def grad_x(self):\n",
    "        pass\n",
    "\n",
    "    def grad_kernel(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjqwKz8xOQzu"
   },
   "source": [
    "#### 1.4 Теперь настало время теста. \n",
    "#### Если вы всё сделали правильно, то запустив следующие ячейки у вас должна появиться надпись: Test PASSED\n",
    "\n",
    "Переходить к дальнейшим заданиям не имеем никакого смысла, пока вы не добьётесь прохождение теста\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYsNSc_2OQzu"
   },
   "source": [
    "#### Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntGUqmZ4OQzu",
    "outputId": "fb16e3a5-c183-4ca0-eafe-663442525e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n",
      "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)  # for reproducibility\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COSgHwsXOQzu"
   },
   "source": [
    "#### Подготовка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "k2QAhXR0OQzv"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "\n",
    "def get_keras_model():\n",
    "    input_image = layers.Input(shape=(1, 28, 28))\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(\n",
    "        input_image\n",
    "    )\n",
    "    flatten = layers.Flatten()(pool1)\n",
    "    dense1 = layers.Dense(10, activation=\"softmax\")(flatten)\n",
    "    model = Model(inputs=input_image, outputs=dense1)\n",
    "\n",
    "    from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, Y_train, validation_split=0.25, batch_size=32, epochs=2, verbose=1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i8mucpQoOQzv"
   },
   "outputs": [],
   "source": [
    "def get_our_model(keras_model):\n",
    "    maxpool = MaxPooling(pool_size=(2, 2), strides=2)\n",
    "    flatten = FlattenLayer()\n",
    "    dense = DenseLayer(\n",
    "        196,\n",
    "        10,\n",
    "        W_init=keras_model.get_weights()[0],\n",
    "        b_init=keras_model.get_weights()[1],\n",
    "    )\n",
    "    softmax = Softmax()\n",
    "    net = Network([maxpool, flatten, dense, softmax])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cxQ33zrOQzv",
    "outputId": "97f75001-fbb3-4051-e8c6-2e736b2a27b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1407/1407 [==============================] - 9s 4ms/step - loss: 0.5695 - accuracy: 0.8494 - val_loss: 0.3838 - val_accuracy: 0.8889\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.3763 - accuracy: 0.8915 - val_loss: 0.3481 - val_accuracy: 0.8983\n"
     ]
    }
   ],
   "source": [
    "keras_model = get_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ik0RKoH_lvCy"
   },
   "outputs": [],
   "source": [
    "our_model = get_our_model(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-nYqgn95Uqy",
    "outputId": "c7b5cac2-4224-4432-d913-dfa4fb5d9302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(keras_model.get_weights()[0].shape)\n",
    "print(keras_model.get_weights()[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Oll9qEYPOQzv"
   },
   "outputs": [],
   "source": [
    "keras_prediction = keras_model.predict(X_test)\n",
    "our_model_prediction = our_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKJsr7HnnUHk",
    "outputId": "f42fed53-61a2-487f-bdde-43df20194b11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9616027e-01, 2.1196460e-08, 4.4861872e-04, 2.1950193e-04,\n",
       "       8.4384692e-06, 1.8447244e-03, 5.3466519e-04, 4.9427280e-04,\n",
       "       1.4624924e-04, 1.4328833e-04], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_prediction[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCVb42jjnZwh",
    "outputId": "9a8fdc55-528f-405c-d77e-e7858e7576c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.96160221e-01, 2.11964139e-08, 4.48618175e-04, 2.19501887e-04,\n",
       "       8.43846736e-06, 1.84472340e-03, 5.34665648e-04, 4.94273076e-04,\n",
       "       1.46249244e-04, 1.43288358e-04])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model_prediction[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuQQLTd4OQzv",
    "outputId": "b5e1b663-988d-4498-ad72-4840c041fc46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "if np.sum(np.abs(keras_prediction - our_model_prediction)) < 0.01:\n",
    "    print(\"Test PASSED\")\n",
    "else:\n",
    "    print(\"Something went wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCuDAHhQOQzv"
   },
   "source": [
    "### 2. Вычисление производных по входу для слоёв нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRJC9_KVOQzw"
   },
   "source": [
    "#### 2.1 (1 балл) Реализуйте метод forward для класса CrossEntropy\n",
    "Напоминание: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
    "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kc67eTlKOQzw"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self, eps=0.00001):\n",
    "        self.name = \"CrossEntropy\"\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input_data, labels):\n",
    "        assert input_data.shape == labels.shape, \"shapes must be equal\"\n",
    "        ans = []\n",
    "        for i in range(input_data.shape[0]):\n",
    "            temp_ans = 0\n",
    "            for j in range(input_data.shape[1]):\n",
    "                temp_ans += -np.log(input_data[i][j] + self.eps) * labels[i][j]\n",
    "            ans.append(temp_ans)\n",
    "        return np.array(ans)\n",
    "\n",
    "    def calculate_loss(self, input_data, labels):\n",
    "        return self.forward(input_data, labels)\n",
    "\n",
    "    def grad_x(self, input_data, labels):\n",
    "        assert input_data.shape == labels.shape, \"shapes must be equal\"\n",
    "        ans = []\n",
    "        for i in range(input_data.shape[0]):\n",
    "            grad = []\n",
    "            for j in range(input_data.shape[1]):\n",
    "                grad.append(-labels[i][j] / input_data[i][j])\n",
    "            ans.append(grad)\n",
    "        return np.array(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Xw8z3MyOQzw"
   },
   "source": [
    "#### 2.2 (2 баллa) Реализуйте метод grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-ffr8LUOQzw"
   },
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "z6l6MTKTOQzw"
   },
   "outputs": [],
   "source": [
    "def numerical_diff_net(net, x, labels):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (\n",
    "            net.calculate_loss(x + delta, labels)\n",
    "            - net.calculate_loss(x - delta, labels)\n",
    "        ) / (2 * eps)\n",
    "        right_answer.append(diff)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "\n",
    "def test_net(net):\n",
    "    x = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "    labels = np.array([[0.3, 0.2, 0.5], [0.1, 0.7, 0.2]])\n",
    "    num_grad = numerical_diff_net(net, x, labels)\n",
    "    grad = net.grad_x(x, labels)\n",
    "    print(\"num_grad shape\", num_grad.shape)\n",
    "    print(\"grad shape\", grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print(\"Test PASSED\")\n",
    "    else:\n",
    "        print(\"Something went wrong!\")\n",
    "        print(\"Numerical grad is\")\n",
    "        print(num_grad)\n",
    "        print(\"Your gradiend is \")\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqNtEA5meaRA",
    "outputId": "bfe80cf0-6c08-42ac-f5d3-1faba8e1b5e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_grad shape (2, 3)\n",
      "grad shape (2, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropy()\n",
    "test_net(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZUt4bpzOQzw"
   },
   "source": [
    "#### 2.3 (2 балла)   Реализуйте метод grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfsJCC_YOQzw"
   },
   "source": [
    "Проверить работоспособность кода поможет следующий тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XKALGKStOQzw"
   },
   "outputs": [],
   "source": [
    "def numerical_diff_layer(layer, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(x[0])):\n",
    "        delta = np.zeros(len(x[0]))\n",
    "        delta[i] = eps\n",
    "        diff = (layer.forward(x + delta) - layer.forward(x - delta)) / (2 * eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "\n",
    "def test_layer(layer):\n",
    "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
    "    num_grad = numerical_diff_layer(layer, x)\n",
    "    grad = layer.grad_x(x)\n",
    "    print(\"num_grad\", num_grad.shape)\n",
    "    print(\"grad\", grad.shape)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print(\"Test PASSED\")\n",
    "    else:\n",
    "        print(\"Something went wrong!\")\n",
    "        print(\"Numerical grad is\")\n",
    "        print(num_grad)\n",
    "        print(\"Your gradiend is \")\n",
    "        print(grad)\n",
    "\n",
    "\n",
    "def random_test_layer(layer):\n",
    "    x = np.random.randn(3, 10)\n",
    "    num_grad = numerical_diff_layer(layer, x)\n",
    "    grad = layer.grad_x(x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print(\"Random Test PASSED\")\n",
    "    else:\n",
    "        print(\"Something went wrong!\")\n",
    "        print(\"Numerical grad is\")\n",
    "        print(num_grad)\n",
    "        print(\"Your gradiend is \")\n",
    "        print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9AjrNHoa-5Z",
    "outputId": "4b56cb03-46c0-41ef-bf5b-0db7868615b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_grad (2, 3, 3)\n",
      "grad (2, 3, 3)\n",
      "Test PASSED\n",
      "Random Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = Softmax()\n",
    "test_layer(layer)\n",
    "random_test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4udeWarOQzw"
   },
   "source": [
    "#### 2.4 (5 баллов) Реализуйте метод grad_x для классов ReLU и DenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okho5LTuOQzw",
    "outputId": "8d69748f-71b1-4121-f26a-31488b790e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_grad (2, 3, 3)\n",
      "grad (2, 3, 3)\n",
      "Test PASSED\n",
      "Random Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = ReLU()\n",
    "test_layer(layer)\n",
    "random_test_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_J0Qho8OQzx",
    "outputId": "07091cbc-8cf8-42e4-f148-52806763906f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_grad (2, 10, 3)\n",
      "grad (2, 10, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "layer = DenseLayer(3, 10)\n",
    "test_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z4TA-q_OQzx"
   },
   "source": [
    "#### 2.5 (4 балла) Для класса Network реализуйте метод grad_x, который должен реализовывать взятие производной от лосса по входу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RikhrD3HOQzx",
    "outputId": "59240510-5ca4-49e0-a2af-df1e20d34241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_grad shape (2, 3)\n",
      "grad shape (2, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "net = Network(\n",
    "    [DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy()\n",
    ")\n",
    "test_net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHLKSukPOQzx"
   },
   "source": [
    "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmKJ1yRnOQzx"
   },
   "source": [
    "#### 3.1 (4 балла) Реализуйте функции grad_b и grad_W. При подготовке теста grad_W предполагается, что W является отномерным вектором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VavduMz1OQzx",
    "outputId": "212d33ba-7253-40c0-facd-5fe5e08c398b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n",
      "\n",
      "Random test begins\n",
      "input_size 62\n",
      "output_size 15\n",
      "x shape (37, 62)\n",
      "Random Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_b(input_size, output_size, b, W, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(len(b)):\n",
    "        delta = np.zeros(b.shape)\n",
    "        delta[i] = eps\n",
    "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b + delta)\n",
    "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b - delta)\n",
    "        diff = (dense1.forward(x) - dense2.forward(x)) / (2 * eps)\n",
    "        right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "\n",
    "def test_grad_b():\n",
    "    input_size = 3\n",
    "    output_size = 4\n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((output_size,))\n",
    "    x = np.random.random((2, input_size))\n",
    "\n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_b(x)\n",
    "\n",
    "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print(\"Test PASSED\")\n",
    "    else:\n",
    "        print(\"Something went wrong!\")\n",
    "        print(\"Numerical grad is\")\n",
    "        print(num_grad)\n",
    "        print(\"Your gradiend is \")\n",
    "        print(grad)\n",
    "\n",
    "\n",
    "def random_test_grad_b():\n",
    "    print(\"\\nRandom test begins\")\n",
    "    input_size = np.random.randint(1, 100)\n",
    "    output_size = np.random.randint(1, 100)\n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((output_size,))\n",
    "    x = np.random.random((np.random.randint(1, 100), input_size))\n",
    "    print(\"input_size\", input_size)\n",
    "    print(\"output_size\", output_size)\n",
    "    print(\"x shape\", x.shape)\n",
    "\n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_b(x)\n",
    "\n",
    "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print(\"Random Test PASSED\")\n",
    "    else:\n",
    "        print(\"Something went wrong!\")\n",
    "        print(\"Numerical grad is\")\n",
    "        print(num_grad)\n",
    "        print(\"Your gradiend is \")\n",
    "        print(grad)\n",
    "\n",
    "\n",
    "test_grad_b()\n",
    "random_test_grad_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8S17Zl1OQzx",
    "outputId": "e36c12dd-1c31-4288-c03d-25f141b8ee7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PASSED\n",
      "\n",
      "Random test begins\n",
      "input_size 12\n",
      "output_size 14\n",
      "x shape (96, 12)\n",
      "Random Test PASSED\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad_W(input_size, output_size, b, W, x):\n",
    "    eps = 0.00001\n",
    "    right_answer = []\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            delta = np.zeros(W.shape)\n",
    "            delta[i, j] = eps\n",
    "            dense1 = DenseLayer(input_size, output_size, W_init=W + delta, b_init=b)\n",
    "            dense2 = DenseLayer(input_size, output_size, W_init=W - delta, b_init=b)\n",
    "            diff = (dense1.forward(x) - dense2.forward(x)) / (2 * eps)\n",
    "            right_answer.append(diff.T)\n",
    "    return np.array(right_answer).T\n",
    "\n",
    "\n",
    "def test_grad_W():\n",
    "    input_size = 3\n",
    "    output_size = 4\n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((output_size,))\n",
    "    x = np.random.random((2, input_size))\n",
    "\n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_W(x)\n",
    "\n",
    "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
    "\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print(\"Test PASSED\")\n",
    "    else:\n",
    "        print(\"Something went wrong!\")\n",
    "        print(\"Numerical grad is\")\n",
    "        print(num_grad)\n",
    "        print(\"Your gradiend is \")\n",
    "        print(grad)\n",
    "\n",
    "\n",
    "def random_test_W():\n",
    "    print(\"\\nRandom test begins\")\n",
    "    input_size = np.random.randint(1, 100)\n",
    "    output_size = np.random.randint(1, 100)\n",
    "    W_init = np.random.random((input_size, output_size))\n",
    "    b_init = np.random.random((output_size,))\n",
    "    x = np.random.random((np.random.randint(1, 100), input_size))\n",
    "\n",
    "    print(\"input_size\", input_size)\n",
    "    print(\"output_size\", output_size)\n",
    "    print(\"x shape\", x.shape)\n",
    "\n",
    "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
    "    grad = dense.grad_W(x)\n",
    "\n",
    "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
    "\n",
    "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
    "        print(\"Random Test PASSED\")\n",
    "    else:\n",
    "        print(\"Something went wrong!\")\n",
    "        print(\"Numerical grad is\")\n",
    "        print(num_grad)\n",
    "        print(\"Your gradiend is \")\n",
    "        print(grad)\n",
    "\n",
    "\n",
    "test_grad_W()\n",
    "random_test_W()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oR4H5UlR7ZWM",
    "outputId": "e253ce0c-b5f8-45fd-980e-ef5a63fc058a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_grad shape (2, 3)\n",
      "grad shape (2, 3)\n",
      "Test PASSED\n"
     ]
    }
   ],
   "source": [
    "net = Network(\n",
    "    [DenseLayer(3, 20), ReLU(), DenseLayer(20, 3), Softmax()], loss=CrossEntropy()\n",
    ")\n",
    "test_net(net)\n",
    "\n",
    "\n",
    "x = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "labels = np.array([[0.3, 0.2, 0.5], [0.1, 0.7, 0.2]])\n",
    "gr = net.grad_param(x, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKRpvam88B_7",
    "outputId": "8fd5ee56-0a34-4370-99ad-9edc2aa368bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights gradient shape = (3, 20)\n",
      "bias gradient shape = (20,)\n",
      "weights gradient shape = (20, 3)\n",
      "bias gradient shape = (3,)\n"
     ]
    }
   ],
   "source": [
    "for elem in gr:\n",
    "    if elem is not None:\n",
    "        print(\"weights gradient shape =\", elem[0].shape)\n",
    "        print(\"bias gradient shape =\", elem[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNdjlGwnOQzy"
   },
   "source": [
    "#### 3.2 (4 балла) Полностью реализуйте метод обратного распространения ошибки в функции train_step класса Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTezmDOkOQzy"
   },
   "source": [
    "Рекомендуем реализовать сначала функцию Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
    "После чего, имея список градиентов, написать функцию обновления параметров для каждого слоя. \n",
    "\n",
    "Совет: рекомендуем написать тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktG8CV8BOQzy"
   },
   "source": [
    "#### 3.3 Ознакомьтесь с реализацией функции fit класса Network. Запустите обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOQYQyDuOQzy",
    "outputId": "2095c12d-5afe-4221-e447-eabde0411f2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:08<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:08<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:07<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:08<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [03:08<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(\n",
    "    trainX[::3],\n",
    "    Y_train[::3],\n",
    "    validation_split=0.25,\n",
    "    batch_size=16,\n",
    "    nb_epoch=5,\n",
    "    learning_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "ac5GfMwrOQzy",
    "outputId": "28659371-2184-4d99-c7cf-7a48602e6c96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:13<00:00,  6.36it/s]\n",
      "  0%|▏                                                                                 | 1/468 [00:00<01:12,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:13<00:00,  6.37it/s]\n",
      "  0%|▏                                                                                 | 1/468 [00:00<01:12,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:13<00:00,  6.40it/s]\n",
      "  0%|▏                                                                                 | 1/468 [00:00<01:11,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:13<00:00,  6.36it/s]\n",
      "  0%|▏                                                                                 | 1/468 [00:00<01:13,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [01:14<00:00,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network(\n",
    "    [DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy()\n",
    ")\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(\n",
    "    trainX[::6],\n",
    "    Y_train[::6],\n",
    "    validation_split=0.25,\n",
    "    batch_size=16,\n",
    "    nb_epoch=5,\n",
    "    learning_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "365cLvN8OQzy"
   },
   "source": [
    "#### 3.5 (2 балла) Продемонстрируйте, что ваша реализация позволяет обучать более глубокие нейронные сети "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0AHtcwoOQzy",
    "outputId": "0565bcca-30fa-4fcf-cedc-910c861384d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [10:07<00:00,  1.30s/it]\n",
      "  0%|                                                                                          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch: val 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [09:36<00:00,  1.23s/it]\n",
      "  0%|                                                                                          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch: val 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [09:19<00:00,  1.19s/it]\n",
      "  0%|                                                                                          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch: val 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [09:17<00:00,  1.19s/it]\n",
      "  0%|                                                                                          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch: val 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 468/468 [15:53<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch: val 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network(\n",
    "    [\n",
    "        DenseLayer(784, 128),\n",
    "        ReLU(),\n",
    "        DenseLayer(128, 64),\n",
    "        ReLU(),\n",
    "        DenseLayer(64, 20),\n",
    "        ReLU(),\n",
    "        DenseLayer(20, 10),\n",
    "        Softmax(),\n",
    "    ],\n",
    "    loss=CrossEntropy(),\n",
    ")\n",
    "trainX = X_train.reshape(len(X_train), -1)\n",
    "net.fit(\n",
    "    trainX[::6],\n",
    "    Y_train[::6],\n",
    "    validation_split=0.25,\n",
    "    batch_size=16,\n",
    "    nb_epoch=5,\n",
    "    learning_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLZ3Jjdhyeac"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practice02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
